Using device: mps

============================================================
제주시 전력 수요 예측 - RNN 계열 모델
논문: 기상 변수 통합 순환 신경망을 활용한 제주시 전력 수요 예측
목표: R² 75% 달성
============================================================
============================================================
데이터 로드 및 전처리 시작...
============================================================
전력 데이터: 4383행, 2013-01-01 00:00:00 ~ 2024-12-31 00:00:00
기온 데이터: 4717행, 2013-01-01 00:00:00 ~ 2025-11-30 00:00:00
일사량 데이터: 4715행
이슬점 온도 데이터: 4717행
관광객 데이터: 4718행
전기차 데이터: 4353행

병합 후 데이터: 4387행, 10열
결측치 현황:
date              0
power_mwh         0
avg_temp          0
min_temp          0
max_temp          0
sunlight          2
dew_point         0
visitors          9
ev_cumulative    30
ev_daily_new     30
dtype: int64

============================================================
이상치 탐지 및 처리 (IQR 기반 Capping)
============================================================
컬럼              하한 이상치       상한 이상치       총계       하한값          상한값         
---------------------------------------------------------------------------
power_mwh       0            1            1        3054.87      16062.59    
visitors        123          7            130      17771.25     56537.25    

총 131개 이상치를 Capping 처리했습니다.

최종 데이터: 4022행, 62열
기간: 2014-01-01 00:00:00 ~ 2024-12-31 00:00:00

============================================================
피어슨 상관계수 분석
============================================================

전력 수요와 변수 간 상관계수 (상위 20개):
--------------------------------------------------
power_rolling_mean_3               : +0.9599  ★★★
power_rolling_max_3                : +0.9502  ★★★
power_rolling_min_3                : +0.9447  ★★★
power_rolling_mean_7               : +0.9260  ★★★
power_lag_1                        : +0.9215  ★★★
power_rolling_max_7                : +0.9097  ★★★
power_rolling_min_7                : +0.9047  ★★★
power_rolling_mean_14              : +0.8966  ★★★
power_rolling_max_14               : +0.8770  ★★★
power_rolling_mean_21              : +0.8756  ★★★
power_rolling_min_14               : +0.8696  ★★★
power_lag_2                        : +0.8624  ★★★
power_rolling_mean_30              : +0.8532  ★★★
power_rolling_max_21               : +0.8521  ★★★
power_rolling_min_21               : +0.8469  ★★★
power_lag_3                        : +0.8412  ★★★
power_rolling_max_30               : +0.8303  ★★★
power_rolling_min_30               : +0.8241  ★★★
power_lag_7                        : +0.8139  ★★★
power_lag_14                       : +0.7644  ★★★
상관관계 히트맵이 '/Users/ibkim/Ormi_1/power-demand-forecast/results/correlation_heatmap.png'에 저장되었습니다.

고상관 피처 (r >= 0.7): 24개

사용할 피처 (27개):
   1. power_rolling_mean_3           (r=+0.960)
   2. power_rolling_max_3            (r=+0.950)
   3. power_rolling_min_3            (r=+0.945)
   4. power_rolling_mean_7           (r=+0.926)
   5. power_lag_1                    (r=+0.921)
   6. power_rolling_max_7            (r=+0.910)
   7. power_rolling_min_7            (r=+0.905)
   8. power_rolling_mean_14          (r=+0.897)
   9. power_rolling_max_14           (r=+0.877)
  10. power_rolling_min_14           (r=+0.870)
  11. power_lag_2                    (r=+0.862)
  12. power_lag_3                    (r=+0.841)
  13. power_lag_7                    (r=+0.814)
  14. year                           (r=+0.732)
  15. power_lag_365                  (r=+0.696)
  16. CDD                            (r=+0.289)
  17. HDD                            (r=+0.268)
  18. avg_temp                       (r=-0.107)
  19. temp_squared                   (r=-0.010)
  20. temp_sunlight                  (r=-0.035)
  21. CDD_sunlight                   (r=+0.265)
  22. extreme_temp                   (r=+0.291)
  23. month_sin                      (r=+0.085)
  24. month_cos                      (r=+0.107)
  25. is_weekend                     (r=-0.112)
  26. dayofweek_sin                  (r=+0.091)
  27. dayofweek_cos                  (r=-0.042)

============================================================
자동 하이퍼파라미터 튜닝 시작 (목표 R²: 0.75)
============================================================

데이터 분할:
  Train: 3291 샘플 (~2022-12-31)
  Val:   365 샘플
  Test:  366 샘플
  피처 수: 27

============================================================
Iteration 1/15 (Seed: 5678)
Config: hidden=64, layers=1, seq=7, batch=32, lr=0.002
============================================================

  Training BiLSTMAttention...
    BiLSTMAttention: MAE=783.67, RMSE=973.20, R²=0.6880 (68.80%)
    ★ New Best R²: 0.6880 (68.80%)

============================================================
Iteration 2/15 (Seed: 5678)
Config: hidden=64, layers=1, seq=7, batch=32, lr=0.001
============================================================

  Training BiLSTMAttention...
    BiLSTMAttention: MAE=753.91, RMSE=949.55, R²=0.7030 (70.30%)
    ★ New Best R²: 0.7030 (70.30%)

============================================================
Iteration 3/15 (Seed: 5678)
Config: hidden=96, layers=1, seq=7, batch=32, lr=0.001
============================================================

  Training BiLSTMAttention...
    BiLSTMAttention: MAE=738.28, RMSE=944.16, R²=0.7063 (70.63%)
    ★ New Best R²: 0.7063 (70.63%)

============================================================
Iteration 4/15 (Seed: 5678)
Config: hidden=64, layers=1, seq=14, batch=32, lr=0.002
============================================================

  Training BiLSTM...
    BiLSTM: MAE=779.46, RMSE=974.48, R²=0.6904 (69.04%)

============================================================
Iteration 5/15 (Seed: 5678)
Config: hidden=64, layers=1, seq=14, batch=32, lr=0.001
============================================================

  Training BiLSTMAttention...
    BiLSTMAttention: MAE=738.30, RMSE=925.11, R²=0.7210 (72.10%)
    ★ New Best R²: 0.7210 (72.10%)

============================================================
Iteration 6/15 (Seed: 5678)
Config: hidden=64, layers=1, seq=21, batch=32, lr=0.001
============================================================

  Training BiLSTM...
    BiLSTM: MAE=745.75, RMSE=949.95, R²=0.7099 (70.99%)

============================================================
Iteration 7/15 (Seed: 5678)
Config: hidden=64, layers=1, seq=21, batch=32, lr=0.001
============================================================

  Training BiLSTMAttention...
    BiLSTMAttention: MAE=768.66, RMSE=968.44, R²=0.6985 (69.85%)

============================================================
Iteration 8/15 (Seed: 5678)
Config: hidden=64, layers=2, seq=7, batch=32, lr=0.001
============================================================

  Training Transformer...
    Transformer: MAE=809.56, RMSE=1006.63, R²=0.6662 (66.62%)

============================================================
Iteration 9/15 (Seed: 5678)
Config: hidden=64, layers=2, seq=14, batch=32, lr=0.0005
============================================================

  Training Transformer...
    Transformer: MAE=808.21, RMSE=992.08, R²=0.6791 (67.91%)

============================================================
Iteration 10/15 (Seed: 5678)
Config: hidden=64, layers=1, seq=7, batch=32, lr=0.002
============================================================

  Training BiLSTM...
    BiLSTM: MAE=744.82, RMSE=940.51, R²=0.7086 (70.86%)

============================================================
Iteration 11/15 (Seed: 5678)
Config: hidden=64, layers=1, seq=7, batch=32, lr=0.002
============================================================

  Training LSTMAttention...
    LSTMAttention: MAE=761.84, RMSE=951.58, R²=0.7017 (70.17%)

============================================================
Iteration 12/15 (Seed: 5678)
Config: hidden=64, layers=1, seq=14, batch=32, lr=0.001
============================================================

  Training LSTMAttention...
    LSTMAttention: MAE=766.53, RMSE=971.76, R²=0.6921 (69.21%)

============================================================
Iteration 13/15 (Seed: 1234)
Config: hidden=64, layers=1, seq=7, batch=32, lr=0.002
============================================================

  Training BiLSTMAttention...
    BiLSTMAttention: MAE=767.31, RMSE=957.08, R²=0.6982 (69.82%)

============================================================
Iteration 14/15 (Seed: 42)
Config: hidden=64, layers=1, seq=7, batch=32, lr=0.002
============================================================

  Training BiLSTMAttention...
    BiLSTMAttention: MAE=814.41, RMSE=1021.26, R²=0.6564 (65.64%)

============================================================
Iteration 15/15 (Seed: 7777)
Config: hidden=64, layers=1, seq=7, batch=32, lr=0.002
============================================================

  Training BiLSTMAttention...
    BiLSTMAttention: MAE=755.07, RMSE=954.18, R²=0.7000 (70.00%)

============================================================
최종 Best R²: 0.7210 (72.10%) - BiLSTMAttention
============================================================

============================================================
앙상블 성능 계산 (9개 모델)
============================================================
  [단순평균] MAE=748.74, RMSE=940.06, R²=0.7089 (70.89%)
    모델 1: R²=0.6880, 가중치=0.110
    모델 2: R²=0.7030, 가중치=0.113
    모델 3: R²=0.7063, 가중치=0.113
    모델 4: R²=0.6662, 가중치=0.107
    모델 5: R²=0.7086, 가중치=0.114
    모델 6: R²=0.7017, 가중치=0.113
    모델 7: R²=0.6982, 가중치=0.112
    모델 8: R²=0.6564, 가중치=0.105
    모델 9: R²=0.7000, 가중치=0.112
  [가중평균] MAE=748.46, RMSE=939.80, R²=0.7090 (70.90%)

  ▶ 최적 앙상블: 가중평균 R²=0.7090 (70.90%)

목표 R² 미달성 - 하이브리드 앙상블 시도...

============================================================
V10: 하이브리드 앙상블 (BiLSTM + Gradient Boosting)
============================================================
Train: 3291, Val: 365, Test: 366

[1] BiLSTM 학습...
  BiLSTM R²: 0.6836 (68.36%)

[2] Gradient Boosting으로 잔차 학습...
  LightGBM 잔차 학습...
  BiLSTM + LightGBM R²: 0.6888 (68.88%)

최종 결과: BiLSTM+LightGBM R²=0.6888 (68.88%)

============================================================
최종 결과
============================================================

최적 모델: BiLSTMAttention

하이퍼파라미터:
  hidden_dim: 64
  num_layers: 1
  seq_length: 14
  batch_size: 32
  lr: 0.001
  dropout: 0.1
  epochs: 700
  seed: 5678
  model: BiLSTMAttention

성능 지표:
  MAE:  738.30 MWh
  MSE:  855828.94
  RMSE: 925.11 MWh
  R²:   0.7210 (72.10%)
  MAPE: 6.41%

============================================================
결과 그래프가 '/Users/ibkim/Ormi_1/power-demand-forecast/results/results.png'에 저장되었습니다.

모델이 '/Users/ibkim/Ormi_1/power-demand-forecast/best_model.pt'에 저장되었습니다.
