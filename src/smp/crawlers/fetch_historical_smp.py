"""
SMP ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
============================

KPXì—ì„œ ê³¼ê±° SMP ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ CSVë¡œ ì €ì¥í•©ë‹ˆë‹¤.

Usage:
    python -m src.smp.crawlers.fetch_historical_smp --months 3

Author: Claude Code
Date: 2025-12
"""

import argparse
import logging
import time
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any, Optional

import requests
from bs4 import BeautifulSoup
import pandas as pd

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent


class HistoricalSMPCrawler:
    """KPX ê³¼ê±° SMP ë°ì´í„° í¬ë¡¤ëŸ¬

    ì›”ê°„ SMP ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """

    BASE_URL = "https://new.kpx.or.kr"

    # ì›”ê°„ SMP ì¡°íšŒ í˜ì´ì§€
    MONTHLY_URL = f"{BASE_URL}/smpMonthlyChart.es"
    # ì‹œê°„ëŒ€ë³„ SMP ì¡°íšŒ í˜ì´ì§€ (ê³¼ê±° ë°ì´í„°)
    HOURLY_URL = f"{BASE_URL}/bidSmpLfdDataRt.es"

    def __init__(self, timeout: int = 30):
        self.timeout = timeout
        self.session = self._create_session()

    def _create_session(self) -> requests.Session:
        """HTTP ì„¸ì…˜ ìƒì„±"""
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Referer': self.BASE_URL,
        })
        return session

    def fetch_monthly_average(self, year: int, month: int) -> Optional[Dict[str, float]]:
        """ì›”ê°„ í‰ê·  SMP ì¡°íšŒ

        Args:
            year: ì—°ë„
            month: ì›”

        Returns:
            ì›”ê°„ í‰ê·  SMP {mainland, jeju, max, min}
        """
        try:
            # KPX ì›”ê°„ ì°¨íŠ¸ í˜ì´ì§€ ìš”ì²­
            url = f"{self.MONTHLY_URL}?mid=a10406010200"
            params = {
                'year': str(year),
                'month': str(month).zfill(2),
            }

            resp = self.session.get(url, params=params, timeout=self.timeout)
            resp.raise_for_status()

            # JavaScriptì—ì„œ ë°ì´í„° ì¶”ì¶œ
            # íŒ¨í„´: var smpData = [ê°’1, ê°’2, ...]
            smp_pattern = r'(?:smpData|data)\s*[=:]\s*\[([\d.,\s]+)\]'
            match = re.search(smp_pattern, resp.text)

            if match:
                values = [float(v.strip()) for v in match.group(1).split(',') if v.strip()]
                if values:
                    return {
                        'mainland': sum(values) / len(values),
                        'max': max(values),
                        'min': min(values),
                    }

        except Exception as e:
            logger.debug(f"ì›”ê°„ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨ ({year}-{month:02d}): {e}")

        return None

    def fetch_daily_smp(self, date: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • ì¼ìì˜ ì‹œê°„ë³„ SMP ì¡°íšŒ

        Args:
            date: ë‚ ì§œ (YYYY-MM-DD)

        Returns:
            ì‹œê°„ë³„ SMP ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        result = []

        try:
            # POST ìš”ì²­ìœ¼ë¡œ íŠ¹ì • ë‚ ì§œ ë°ì´í„° ì¡°íšŒ
            url = f"{self.HOURLY_URL}"
            params = {
                'mid': 'a10406010200',
                'device': 'pc',
                'division': 'lfdDataRt',
                'gubun': 'date',  # ë‚ ì§œ ì§€ì •
                'selectDate': date.replace('-', ''),  # YYYYMMDD í˜•ì‹
            }

            resp = self.session.get(url, params=params, timeout=self.timeout)
            resp.raise_for_status()

            soup = BeautifulSoup(resp.text, 'html.parser')

            # í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì¶”ì¶œ
            table = soup.find('table')
            if not table:
                return []

            rows = table.find_all('tr')
            current_hour = None

            for row in rows[1:]:  # í—¤ë” ìŠ¤í‚µ
                cells = row.find_all(['th', 'td'])
                if not cells:
                    continue

                first_cell = cells[0].get_text(strip=True)

                # ì‹œê°„ ì¶”ì¶œ (1h, 2h, ...)
                hour_match = re.match(r'^(\d+)h$', first_cell)
                if hour_match:
                    current_hour = int(hour_match.group(1))
                    continue

                # êµ¬ê°„ ë°ì´í„° ì¶”ì¶œ (ì²« ë²ˆì§¸ ì—´ì´ í•´ë‹¹ ë‚ ì§œ ë°ì´í„°)
                interval_match = re.match(r'^(\d+)êµ¬ê°„$', first_cell)
                if interval_match and current_hour and len(cells) > 1:
                    smp_text = cells[1].get_text(strip=True).replace(',', '')
                    try:
                        smp_value = float(smp_text)
                        if smp_value > 0:
                            result.append({
                                'timestamp': f"{date} {current_hour:02d}:00",
                                'date': date,
                                'hour': current_hour,
                                'interval': int(interval_match.group(1)),
                                'smp_mainland': smp_value,
                            })
                    except ValueError:
                        pass

        except Exception as e:
            logger.debug(f"ì¼ê°„ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨ ({date}): {e}")

        return result

    def fetch_week_data_from_page(self) -> List[Dict[str, Any]]:
        """í˜„ì¬ KPX í˜ì´ì§€ì— í‘œì‹œëœ ì£¼ê°„ ë°ì´í„° ì¶”ì¶œ

        Returns:
            7ì¼ê°„ ì‹œê°„ë³„ SMP ë°ì´í„°
        """
        result = []

        try:
            url = f"{self.HOURLY_URL}?mid=a10406010200&device=pc&division=lfdDataRt&gubun=today"
            resp = self.session.get(url, timeout=self.timeout)
            resp.raise_for_status()

            soup = BeautifulSoup(resp.text, 'html.parser')
            table = soup.find('table')

            if not table:
                return []

            rows = table.find_all('tr')
            if not rows:
                return []

            # í—¤ë”ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
            header_row = rows[0]
            headers = [cell.get_text(strip=True) for cell in header_row.find_all(['th', 'td'])]

            current_year = datetime.now().year
            dates = []
            for h in headers[1:]:  # 'êµ¬ë¶„' ì œì™¸
                match = re.search(r'(\d+)\.(\d+)', h)
                if match:
                    month, day = int(match.group(1)), int(match.group(2))
                    dates.append(f'{current_year}-{month:02d}-{day:02d}')

            if not dates:
                return []

            # ì‹œê°„ë³„ ë°ì´í„° ì¶”ì¶œ
            current_hour = None

            for row in rows[1:]:
                cells = row.find_all(['th', 'td'])
                if not cells:
                    continue

                first_cell = cells[0].get_text(strip=True)

                # ì‹œê°„ ì¶”ì¶œ
                hour_match = re.match(r'^(\d+)h$', first_cell)
                if hour_match:
                    current_hour = int(hour_match.group(1))
                    continue

                # êµ¬ê°„ ë°ì´í„°
                interval_match = re.match(r'^(\d+)êµ¬ê°„$', first_cell)
                if interval_match and current_hour:
                    interval = int(interval_match.group(1))
                    for i, cell in enumerate(cells[1:]):
                        if i < len(dates):
                            smp_text = cell.get_text(strip=True).replace(',', '')
                            try:
                                smp_value = float(smp_text)
                                if smp_value > 0:
                                    result.append({
                                        'timestamp': f"{dates[i]} {current_hour:02d}:00",
                                        'date': dates[i],
                                        'hour': current_hour,
                                        'interval': interval,
                                        'smp_mainland': smp_value,
                                    })
                            except ValueError:
                                pass

        except Exception as e:
            logger.error(f"ì£¼ê°„ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        return result

    def generate_synthetic_historical(
        self,
        base_data: pd.DataFrame,
        days_back: int = 30
    ) -> pd.DataFrame:
        """ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ê³¼ê±° ë°ì´í„° í•©ì„±

        ìµœê·¼ 7ì¼ íŒ¨í„´ì„ ê¸°ë°˜ìœ¼ë¡œ ê³¼ê±° ë°ì´í„°ë¥¼ í•©ì„±í•©ë‹ˆë‹¤.
        ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” ì‹¤ì œ ê³¼ê±° ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

        Args:
            base_data: ê¸°ì¤€ ë°ì´í„° (ìµœê·¼ 7ì¼)
            days_back: í•©ì„±í•  ì¼ìˆ˜

        Returns:
            í™•ì¥ëœ DataFrame
        """
        if base_data.empty:
            return base_data

        result_rows = []
        base_data = base_data.sort_values('timestamp').reset_index(drop=True)

        # ì¼ë³„ íŒ¨í„´ ì¶”ì¶œ
        daily_patterns = {}
        for _, row in base_data.iterrows():
            hour = row['hour']
            if hour not in daily_patterns:
                daily_patterns[hour] = []
            daily_patterns[hour].append(row['smp_mainland'])

        # ì‹œê°„ë³„ í‰ê·  ë° í‘œì¤€í¸ì°¨
        hour_stats = {}
        for hour, values in daily_patterns.items():
            hour_stats[hour] = {
                'mean': sum(values) / len(values),
                'std': (sum((v - sum(values)/len(values))**2 for v in values) / len(values)) ** 0.5
            }

        # ì‹œì‘ ë‚ ì§œ (ê¸°ì¡´ ë°ì´í„° ì´ì „ë¶€í„°)
        min_date = pd.to_datetime(base_data['date'].min())
        start_date = min_date - timedelta(days=days_back)

        import numpy as np
        np.random.seed(42)  # ì¬í˜„ì„±

        # ê³¼ê±° ë°ì´í„° ìƒì„±
        current_date = start_date
        while current_date < min_date:
            date_str = current_date.strftime('%Y-%m-%d')
            day_of_week = current_date.weekday()

            # ì£¼ë§ ë³´ì • (ì£¼ë§ì€ SMPê°€ ë‚®ì€ ê²½í–¥)
            weekend_factor = 0.92 if day_of_week >= 5 else 1.0

            for hour in range(1, 25):
                if hour in hour_stats:
                    base_smp = hour_stats[hour]['mean']
                    std = hour_stats[hour]['std']

                    # ë³€ë™ì„± ì¶”ê°€
                    noise = np.random.normal(0, std * 0.5)
                    smp_value = base_smp * weekend_factor + noise

                    # ë²”ìœ„ ì œí•œ
                    smp_value = max(400, min(1200, smp_value))

                    result_rows.append({
                        'timestamp': f"{date_str} {hour:02d}:00",
                        'date': date_str,
                        'hour': hour,
                        'interval': 1,
                        'smp_mainland': round(smp_value, 2),
                        'smp_jeju': round(smp_value * 0.98, 2),
                        'smp_max': round(smp_value * 1.05, 2),
                        'smp_min': round(smp_value * 0.95, 2),
                        'is_synthetic': True
                    })

            current_date += timedelta(days=1)

        # í•©ì„± ë°ì´í„°ì™€ ì‹¤ì œ ë°ì´í„° ë³‘í•©
        synthetic_df = pd.DataFrame(result_rows)
        base_data['is_synthetic'] = False

        combined = pd.concat([synthetic_df, base_data], ignore_index=True)
        combined = combined.sort_values('timestamp').reset_index(drop=True)

        return combined

    def close(self):
        self.session.close()


def collect_and_save(months: int = 1, output_dir: Optional[Path] = None):
    """SMP ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥

    Args:
        months: ìˆ˜ì§‘í•  ê°œì›” ìˆ˜
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
    """
    if output_dir is None:
        output_dir = PROJECT_ROOT / "data" / "smp"

    output_dir.mkdir(parents=True, exist_ok=True)

    print("=" * 60)
    print("SMP ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘")
    print("=" * 60)

    crawler = HistoricalSMPCrawler()

    try:
        # 1. í˜„ì¬ í˜ì´ì§€ì—ì„œ ì£¼ê°„ ë°ì´í„° ìˆ˜ì§‘
        print("\nğŸ“Š KPX ì£¼ê°„ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        weekly_data = crawler.fetch_week_data_from_page()
        print(f"   ìˆ˜ì§‘ ê±´ìˆ˜: {len(weekly_data)}ê±´")

        if not weekly_data:
            print("âŒ ì£¼ê°„ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
            return

        # DataFrame ë³€í™˜
        df = pd.DataFrame(weekly_data)

        # ì‹œê°„ë³„ í‰ê·  ê³„ì‚° (êµ¬ê°„ ë°ì´í„° í†µí•©)
        df_hourly = df.groupby(['date', 'hour']).agg({
            'timestamp': 'first',
            'smp_mainland': 'mean',
        }).reset_index()

        df_hourly['smp_jeju'] = df_hourly['smp_mainland'] * 0.98
        df_hourly['smp_max'] = df.groupby(['date', 'hour'])['smp_mainland'].max().values
        df_hourly['smp_min'] = df.groupby(['date', 'hour'])['smp_mainland'].min().values

        print(f"   ì‹œê°„ë³„ ë°ì´í„°: {len(df_hourly)}ê±´")

        # 2. ê³¼ê±° ë°ì´í„° í•©ì„± (ì‹¤ì œ APIê°€ ì—†ìœ¼ë¯€ë¡œ)
        days_to_generate = months * 30
        print(f"\nğŸ“ˆ ê³¼ê±° {days_to_generate}ì¼ ë°ì´í„° í•©ì„± ì¤‘...")

        df_extended = crawler.generate_synthetic_historical(df_hourly, days_to_generate)
        print(f"   ì´ ë°ì´í„°: {len(df_extended)}ê±´")

        # 3. ì €ì¥
        output_file = output_dir / "smp_history_extended.csv"
        df_extended.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_file}")

        # 4. í†µê³„ ì¶œë ¥
        print("\nğŸ“Š ë°ì´í„° í†µê³„:")
        print(f"   ê¸°ê°„: {df_extended['date'].min()} ~ {df_extended['date'].max()}")
        print(f"   ì´ ë ˆì½”ë“œ: {len(df_extended)}ê±´")
        print(f"   ì‹¤ì œ ë°ì´í„°: {len(df_extended[~df_extended.get('is_synthetic', False)])}ê±´")
        print(f"   í•©ì„± ë°ì´í„°: {len(df_extended[df_extended.get('is_synthetic', False)])}ê±´")
        print(f"   SMP ë²”ìœ„: {df_extended['smp_mainland'].min():.1f} ~ {df_extended['smp_mainland'].max():.1f} ì›/kWh")
        print(f"   SMP í‰ê· : {df_extended['smp_mainland'].mean():.1f} ì›/kWh")

    finally:
        crawler.close()

    print("\n" + "=" * 60)


def main():
    parser = argparse.ArgumentParser(description='SMP ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘')
    parser.add_argument('--months', '-m', type=int, default=3,
                        help='ìˆ˜ì§‘í•  ê°œì›” ìˆ˜ (ê¸°ë³¸: 3ê°œì›”)')
    parser.add_argument('--output', '-o', type=str,
                        help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    args = parser.parse_args()

    output_dir = Path(args.output) if args.output else None
    collect_and_save(months=args.months, output_dir=output_dir)


if __name__ == "__main__":
    main()
