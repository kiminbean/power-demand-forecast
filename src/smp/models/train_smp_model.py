"""
SMP LSTM ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
===========================

ì‹¤ì œ KPX SMP ë°ì´í„°ë¡œ LSTM ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.

Usage:
    python -m src.smp.models.train_smp_model

Author: Claude Code
Date: 2025-12
"""

import os
import sys
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Tuple, Dict, Any, Optional

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from src.smp.models.smp_lstm import SMPLSTMModel, get_device, model_summary

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SMPDataset(Dataset):
    """SMP ì‹œê³„ì—´ ë°ì´í„°ì…‹

    Args:
        data: SMP ì‹œê³„ì—´ ë°ì´í„° (ì •ê·œí™”ëœ numpy array)
        input_hours: ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´
        output_hours: ì¶œë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´
        step: ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìŠ¤í…
    """

    def __init__(
        self,
        data: np.ndarray,
        input_hours: int = 24,
        output_hours: int = 24,
        step: int = 1
    ):
        self.data = torch.FloatTensor(data)
        self.input_hours = input_hours
        self.output_hours = output_hours
        self.step = step

        # ìƒ˜í”Œ ì¸ë±ìŠ¤ ìƒì„±
        total_len = len(data)
        self.indices = []
        for i in range(0, total_len - input_hours - output_hours + 1, step):
            self.indices.append(i)

    def __len__(self):
        return len(self.indices)

    def __getitem__(self, idx):
        start = self.indices[idx]
        x = self.data[start:start + self.input_hours]
        y = self.data[start + self.input_hours:start + self.input_hours + self.output_hours, 0]  # SMP only
        return x, y


def load_smp_data(data_path: str) -> pd.DataFrame:
    """SMP ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬

    Args:
        data_path: CSV íŒŒì¼ ê²½ë¡œ

    Returns:
        ì „ì²˜ë¦¬ëœ DataFrame
    """
    df = pd.read_csv(data_path)

    # 0ì´ ì•„ë‹Œ ìœ íš¨ ë°ì´í„°ë§Œ í•„í„°ë§
    df = df[df['smp_mainland'] > 0].copy()

    # 24:00 â†’ 00:00 ë³€í™˜ (ë‹¤ìŒë‚ ë¡œ ì²˜ë¦¬)
    def fix_hour_24(timestamp):
        if ' 24:00' in str(timestamp):
            # 24:00ì„ ë‹¤ìŒë‚  00:00ìœ¼ë¡œ ë³€í™˜
            date_part = str(timestamp).replace(' 24:00', '')
            dt = pd.to_datetime(date_part) + pd.Timedelta(days=1)
            return dt
        return pd.to_datetime(timestamp)

    df['datetime'] = df['timestamp'].apply(fix_hour_24)

    # ì‹œê°„ìˆœ ì •ë ¬
    df = df.sort_values('datetime').reset_index(drop=True)

    logger.info(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê±´")
    logger.info(f"ê¸°ê°„: {df['datetime'].min()} ~ {df['datetime'].max()}")

    return df


def create_features(df: pd.DataFrame) -> np.ndarray:
    """í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§

    Args:
        df: SMP DataFrame

    Returns:
        í”¼ì²˜ ë°°ì—´ (n_samples, n_features)
    """
    features = []

    # ê¸°ë³¸ SMP í”¼ì²˜
    features.append(df['smp_mainland'].values)  # ìœ¡ì§€ SMP
    features.append(df['smp_jeju'].values)      # ì œì£¼ SMP
    features.append(df['smp_max'].values)       # ìµœê³ ê°€
    features.append(df['smp_min'].values)       # ìµœì €ê°€

    # ì‹œê°„ í”¼ì²˜
    hour = df['hour'].values
    features.append(np.sin(2 * np.pi * hour / 24))  # ì‹œê°„ ì‚¬ì¸
    features.append(np.cos(2 * np.pi * hour / 24))  # ì‹œê°„ ì½”ì‚¬ì¸

    # ìš”ì¼ í”¼ì²˜ (datetimeì—ì„œ ì¶”ì¶œ)
    day_of_week = df['datetime'].dt.dayofweek.values
    features.append(np.sin(2 * np.pi * day_of_week / 7))  # ìš”ì¼ ì‚¬ì¸
    features.append(np.cos(2 * np.pi * day_of_week / 7))  # ìš”ì¼ ì½”ì‚¬ì¸

    # ì£¼ë§ ì—¬ë¶€
    features.append((day_of_week >= 5).astype(float))  # ì£¼ë§ = 1

    # SMP ë³€í™”ìœ¨ (lag features)
    smp = df['smp_mainland'].values
    smp_diff = np.diff(smp, prepend=smp[0])
    features.append(smp_diff)  # SMP ë³€í™”ëŸ‰

    # ì´ë™ í‰ê·  (ì§§ì€ ë°ì´í„°ì— ë§ì¶¤)
    smp_ma3 = pd.Series(smp).rolling(3, min_periods=1).mean().values
    smp_ma6 = pd.Series(smp).rolling(6, min_periods=1).mean().values
    features.append(smp_ma3)
    features.append(smp_ma6)

    # ìŠ¤íƒ
    feature_array = np.column_stack(features)

    logger.info(f"í”¼ì²˜ ìƒì„± ì™„ë£Œ: {feature_array.shape}")

    return feature_array


def normalize_data(
    data: np.ndarray,
    scaler: Optional[MinMaxScaler] = None
) -> Tuple[np.ndarray, MinMaxScaler]:
    """ë°ì´í„° ì •ê·œí™”

    Args:
        data: ì…ë ¥ ë°ì´í„°
        scaler: ê¸°ì¡´ ìŠ¤ì¼€ì¼ëŸ¬ (ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±)

    Returns:
        ì •ê·œí™”ëœ ë°ì´í„°, ìŠ¤ì¼€ì¼ëŸ¬
    """
    if scaler is None:
        scaler = MinMaxScaler(feature_range=(0, 1))
        normalized = scaler.fit_transform(data)
    else:
        normalized = scaler.transform(data)

    return normalized, scaler


def train_model(
    model: nn.Module,
    train_loader: DataLoader,
    val_loader: DataLoader,
    epochs: int = 100,
    learning_rate: float = 0.001,
    patience: int = 15,
    device: torch.device = None
) -> Dict[str, Any]:
    """ëª¨ë¸ í•™ìŠµ

    Args:
        model: í•™ìŠµí•  ëª¨ë¸
        train_loader: í•™ìŠµ ë°ì´í„° ë¡œë”
        val_loader: ê²€ì¦ ë°ì´í„° ë¡œë”
        epochs: ìµœëŒ€ ì—í­ ìˆ˜
        learning_rate: í•™ìŠµë¥ 
        patience: Early stopping patience
        device: í•™ìŠµ ë””ë°”ì´ìŠ¤

    Returns:
        í•™ìŠµ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    if device is None:
        device = get_device()

    model = model.to(device)

    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5
    )

    history = {'train_loss': [], 'val_loss': [], 'val_mape': []}
    best_val_loss = float('inf')
    best_model_state = None
    patience_counter = 0

    logger.info(f"í•™ìŠµ ì‹œì‘ (device: {device})")
    logger.info(f"í•™ìŠµ ìƒ˜í”Œ: {len(train_loader.dataset)}, ê²€ì¦ ìƒ˜í”Œ: {len(val_loader.dataset)}")

    for epoch in range(epochs):
        # Training
        model.train()
        train_loss = 0.0

        for batch_x, batch_y in train_loader:
            batch_x = batch_x.to(device)
            batch_y = batch_y.to(device)

            optimizer.zero_grad()
            output = model(batch_x)
            loss = criterion(output, batch_y)
            loss.backward()

            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()
            train_loss += loss.item()

        train_loss /= len(train_loader)

        # Validation
        model.eval()
        val_loss = 0.0
        val_mape = 0.0

        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x = batch_x.to(device)
                batch_y = batch_y.to(device)

                output = model(batch_x)
                loss = criterion(output, batch_y)
                val_loss += loss.item()

                # MAPE ê³„ì‚° (ì •ê·œí™”ëœ ê°’ ê¸°ì¤€)
                mape = torch.mean(torch.abs((batch_y - output) / (batch_y + 1e-8))) * 100
                val_mape += mape.item()

        val_loss /= len(val_loader)
        val_mape /= len(val_loader)

        # Learning rate ì¡°ì •
        scheduler.step(val_loss)

        # History ê¸°ë¡
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['val_mape'].append(val_mape)

        # ë¡œê¹…
        if (epoch + 1) % 10 == 0 or epoch == 0:
            logger.info(
                f"Epoch {epoch+1:3d}/{epochs} | "
                f"Train Loss: {train_loss:.6f} | "
                f"Val Loss: {val_loss:.6f} | "
                f"Val MAPE: {val_mape:.2f}%"
            )

        # Early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model_state = model.state_dict().copy()
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                logger.info(f"Early stopping at epoch {epoch+1}")
                break

    # Best ëª¨ë¸ ë³µì›
    if best_model_state is not None:
        model.load_state_dict(best_model_state)

    return {
        'history': history,
        'best_val_loss': best_val_loss,
        'epochs_trained': epoch + 1
    }


def evaluate_model(
    model: nn.Module,
    test_loader: DataLoader,
    scaler: MinMaxScaler,
    device: torch.device = None
) -> Dict[str, float]:
    """ëª¨ë¸ í‰ê°€

    Args:
        model: í‰ê°€í•  ëª¨ë¸
        test_loader: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”
        scaler: ì—­ì •ê·œí™”ìš© ìŠ¤ì¼€ì¼ëŸ¬
        device: ë””ë°”ì´ìŠ¤

    Returns:
        í‰ê°€ ì§€í‘œ ë”•ì…”ë„ˆë¦¬
    """
    if device is None:
        device = get_device()

    model = model.to(device)
    model.eval()

    all_preds = []
    all_targets = []

    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x = batch_x.to(device)
            output = model(batch_x)

            all_preds.append(output.cpu().numpy())
            all_targets.append(batch_y.numpy())

    preds = np.concatenate(all_preds, axis=0)
    targets = np.concatenate(all_targets, axis=0)

    # ì—­ì •ê·œí™” (ì²« ë²ˆì§¸ í”¼ì²˜ê°€ SMP)
    # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ì—­ì •ê·œí™”
    smp_min = scaler.data_min_[0]
    smp_max = scaler.data_max_[0]
    smp_range = smp_max - smp_min

    preds_real = preds * smp_range + smp_min
    targets_real = targets * smp_range + smp_min

    # í‰ê°€ ì§€í‘œ
    mae = np.mean(np.abs(preds_real - targets_real))
    rmse = np.sqrt(np.mean((preds_real - targets_real) ** 2))
    mape = np.mean(np.abs((targets_real - preds_real) / (targets_real + 1e-8))) * 100

    # RÂ² Score
    ss_res = np.sum((targets_real - preds_real) ** 2)
    ss_tot = np.sum((targets_real - np.mean(targets_real)) ** 2)
    r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0

    return {
        'mae': float(mae),
        'rmse': float(rmse),
        'mape': float(mape),
        'r2': float(r2)
    }


def save_model(
    model: nn.Module,
    scaler: MinMaxScaler,
    metrics: Dict[str, Any],
    config: Dict[str, Any],
    output_dir: str
):
    """ëª¨ë¸ ì €ì¥

    Args:
        model: ì €ì¥í•  ëª¨ë¸
        scaler: ìŠ¤ì¼€ì¼ëŸ¬
        metrics: í‰ê°€ ì§€í‘œ
        config: ëª¨ë¸ ì„¤ì •
        output_dir: ì €ì¥ ë””ë ‰í† ë¦¬
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # ëª¨ë¸ ì €ì¥
    model_path = output_path / 'smp_lstm_model.pt'
    torch.save({
        'model_state_dict': model.state_dict(),
        'config': config,
        'metrics': metrics,
        'timestamp': datetime.now().isoformat()
    }, model_path)

    # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    scaler_path = output_path / 'smp_scaler.npy'
    np.save(scaler_path, {
        'min_': scaler.data_min_,
        'max_': scaler.data_max_,
        'scale_': scaler.scale_,
        'data_min_': scaler.data_min_,
        'data_max_': scaler.data_max_,
        'feature_range': scaler.feature_range
    })

    # ë©”íŠ¸ë¦­ ì €ì¥
    metrics_path = output_path / 'smp_metrics.json'
    with open(metrics_path, 'w') as f:
        json.dump({
            'metrics': metrics,
            'config': config,
            'timestamp': datetime.now().isoformat()
        }, f, indent=2)

    logger.info(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_path}")


def main():
    """ë©”ì¸ í•™ìŠµ í•¨ìˆ˜"""
    print("=" * 60)
    print("SMP LSTM ëª¨ë¸ í•™ìŠµ")
    print("=" * 60)

    # ì„¤ì •
    config = {
        'data_path': 'data/smp/smp_history_real.csv',
        'output_dir': 'models/smp',
        'input_hours': 24,      # 24ì‹œê°„ ì…ë ¥
        'output_hours': 24,     # 24ì‹œê°„ ì˜ˆì¸¡
        'hidden_size': 64,      # ì‘ì€ ë°ì´í„°ì…‹ì— ë§ì¶¤
        'num_layers': 2,
        'dropout': 0.3,         # ê°•í•œ regularization
        'bidirectional': True,
        'batch_size': 8,
        'epochs': 150,
        'learning_rate': 0.001,
        'patience': 20,
        'test_size': 0.2,
        'val_size': 0.2,
    }

    print("\nğŸ“‹ ì„¤ì •:")
    for k, v in config.items():
        print(f"   {k}: {v}")

    # 1. ë°ì´í„° ë¡œë“œ
    print("\n" + "=" * 60)
    print("1. ë°ì´í„° ë¡œë“œ")
    print("=" * 60)
    df = load_smp_data(config['data_path'])

    # 2. í”¼ì²˜ ìƒì„±
    print("\n" + "=" * 60)
    print("2. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§")
    print("=" * 60)
    features = create_features(df)
    print(f"í”¼ì²˜ ìˆ˜: {features.shape[1]}")

    # 3. ì •ê·œí™”
    print("\n" + "=" * 60)
    print("3. ë°ì´í„° ì •ê·œí™”")
    print("=" * 60)
    normalized_data, scaler = normalize_data(features)
    print(f"ì •ê·œí™” ì™„ë£Œ: {normalized_data.shape}")

    # 4. ë°ì´í„°ì…‹ ë¶„í• 
    print("\n" + "=" * 60)
    print("4. ë°ì´í„°ì…‹ ë¶„í• ")
    print("=" * 60)

    # ì‹œê³„ì—´ì´ë¯€ë¡œ ìˆœì°¨ ë¶„í• 
    n_samples = len(normalized_data)
    train_end = int(n_samples * (1 - config['test_size'] - config['val_size']))
    val_end = int(n_samples * (1 - config['test_size']))

    train_data = normalized_data[:train_end]
    val_data = normalized_data[train_end:val_end]
    test_data = normalized_data[val_end:]

    print(f"í•™ìŠµ: {len(train_data)}, ê²€ì¦: {len(val_data)}, í…ŒìŠ¤íŠ¸: {len(test_data)}")

    # 5. ë°ì´í„°ë¡œë” ìƒì„± (ì‘ì€ ë°ì´í„°ì…‹ì— ë§ì¶¤)
    # ì „ì²´ ë°ì´í„°ë¡œ í•˜ë‚˜ì˜ ë°ì´í„°ì…‹ ìƒì„±
    full_dataset = SMPDataset(normalized_data, config['input_hours'], config['output_hours'], step=1)
    n_samples = len(full_dataset)
    print(f"ì „ì²´ ìƒ˜í”Œ ìˆ˜: {n_samples}")

    if n_samples < 20:
        logger.warning("ë°ì´í„°ê°€ ë§¤ìš° ì ìŠµë‹ˆë‹¤. êµì°¨ ê²€ì¦ ì—†ì´ í•™ìŠµí•©ë‹ˆë‹¤.")
        train_dataset = full_dataset
        val_dataset = full_dataset
        test_dataset = full_dataset
    else:
        # ì‹œê³„ì—´ ìˆœì„œëŒ€ë¡œ ë¶„í• 
        train_size = int(n_samples * 0.7)
        val_size = int(n_samples * 0.15)

        # ì¸ë±ìŠ¤ ê¸°ë°˜ ë¶„í• 
        train_indices = list(range(train_size))
        val_indices = list(range(train_size, train_size + val_size))
        test_indices = list(range(train_size + val_size, n_samples))

        # Subsetìœ¼ë¡œ ë¶„í• 
        from torch.utils.data import Subset
        train_dataset = Subset(full_dataset, train_indices)
        val_dataset = Subset(full_dataset, val_indices) if val_indices else train_dataset
        test_dataset = Subset(full_dataset, test_indices) if test_indices else train_dataset

    print(f"í•™ìŠµ ìƒ˜í”Œ: {len(train_dataset)}, ê²€ì¦ ìƒ˜í”Œ: {len(val_dataset)}, í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(test_dataset)}")

    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)

    # 6. ëª¨ë¸ ìƒì„±
    print("\n" + "=" * 60)
    print("5. ëª¨ë¸ ìƒì„±")
    print("=" * 60)

    input_size = features.shape[1]
    model = SMPLSTMModel(
        input_size=input_size,
        hidden_size=config['hidden_size'],
        num_layers=config['num_layers'],
        dropout=config['dropout'],
        bidirectional=config['bidirectional'],
        prediction_hours=config['output_hours']
    )

    print(model_summary(model))

    device = get_device()
    print(f"Device: {device}")

    # 7. ëª¨ë¸ í•™ìŠµ
    print("\n" + "=" * 60)
    print("6. ëª¨ë¸ í•™ìŠµ")
    print("=" * 60)

    train_result = train_model(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        epochs=config['epochs'],
        learning_rate=config['learning_rate'],
        patience=config['patience'],
        device=device
    )

    print(f"\ní•™ìŠµ ì™„ë£Œ: {train_result['epochs_trained']} epochs")
    print(f"Best Val Loss: {train_result['best_val_loss']:.6f}")

    # 8. ëª¨ë¸ í‰ê°€
    print("\n" + "=" * 60)
    print("7. ëª¨ë¸ í‰ê°€")
    print("=" * 60)

    metrics = evaluate_model(model, test_loader, scaler, device)

    print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"   MAE:  {metrics['mae']:.2f} ì›/kWh")
    print(f"   RMSE: {metrics['rmse']:.2f} ì›/kWh")
    print(f"   MAPE: {metrics['mape']:.2f}%")
    print(f"   RÂ²:   {metrics['r2']:.4f}")

    # 9. ëª¨ë¸ ì €ì¥
    print("\n" + "=" * 60)
    print("8. ëª¨ë¸ ì €ì¥")
    print("=" * 60)

    config['input_size'] = input_size
    save_model(model, scaler, metrics, config, config['output_dir'])

    # 10. ì˜ˆì¸¡ ìƒ˜í”Œ ì¶œë ¥
    print("\n" + "=" * 60)
    print("9. ì˜ˆì¸¡ ìƒ˜í”Œ")
    print("=" * 60)

    model.eval()
    with torch.no_grad():
        sample_x, sample_y = test_dataset[0]
        sample_x = sample_x.unsqueeze(0).to(device)
        pred = model(sample_x).cpu().numpy()[0]
        actual = sample_y.numpy()

        # ì—­ì •ê·œí™”
        smp_min = scaler.data_min_[0]
        smp_max = scaler.data_max_[0]
        smp_range = smp_max - smp_min

        pred_real = pred * smp_range + smp_min
        actual_real = actual * smp_range + smp_min

        print("\nì‹œê°„ë³„ ì˜ˆì¸¡ vs ì‹¤ì œ (ì²« ìƒ˜í”Œ):")
        print("-" * 50)
        for h in range(min(12, len(pred_real))):  # ì²˜ìŒ 12ì‹œê°„ë§Œ
            error = abs(pred_real[h] - actual_real[h])
            print(f"  {h+1:2d}ì‹œ: ì˜ˆì¸¡ {pred_real[h]:7.2f} | ì‹¤ì œ {actual_real[h]:7.2f} | ì˜¤ì°¨ {error:6.2f}")

    print("\n" + "=" * 60)
    print("í•™ìŠµ ì™„ë£Œ!")
    print("=" * 60)

    return model, scaler, metrics


if __name__ == "__main__":
    main()
