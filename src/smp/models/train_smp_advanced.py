"""
SMP Advanced Training Pipeline - Sim-to-Real Architecture
==========================================================

ìˆ˜ì„ ì•„í‚¤í…íŠ¸ ì œì–¸ì„ ë°˜ì˜í•œ ê³ ë„í™”ëœ SMP ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸

í•µì‹¬ ì„¤ê³„ ì›ì¹™:
1. Transfer Learning: í•©ì„± ë°ì´í„° Pre-train â†’ ì‹¤ì œ ë°ì´í„° Fine-tune
2. ê²½ëŸ‰í™” ëª¨ë¸: íŒŒë¼ë¯¸í„° 1/5 ìˆ˜ì¤€ìœ¼ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ ê°•í™”
3. Quantile Regression: ë¶ˆí™•ì‹¤ì„± ì¶”ì • (10%, 50%, 90%)
4. Walk-forward Validation: ì‹œê³„ì—´ êµì°¨ê²€ì¦
5. Drift Detection: í•©ì„±â†’ì‹¤ì œ ì„±ëŠ¥ ë“œë¦¬í”„íŠ¸ ì¸¡ì •
6. Noise Injection: ë°ì´í„° ì¦ê°•ì„ í†µí•œ ë¡œë²„ìŠ¤íŠ¸ì„± í™•ë³´
7. ARIMA Ensemble: í†µê³„ ëª¨ë¸ê³¼ì˜ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼
8. XAI Pipeline: Attention ê¸°ë°˜ í•´ì„ ê°€ëŠ¥ì„±

Usage:
    python -m src.smp.models.train_smp_advanced

Author: Claude Code (Superintelligent AI/ML Specialist)
Date: 2025-12
"""

import os
import sys
import json
import logging
import warnings
from datetime import datetime
from pathlib import Path
from typing import Tuple, Dict, Any, Optional, List
from dataclasses import dataclass, field

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, Subset
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt

warnings.filterwarnings('ignore')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from src.smp.models.smp_lstm import get_device

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)


# =============================================================================
# Configuration
# =============================================================================
@dataclass
class TrainingConfig:
    """í•™ìŠµ ì„¤ì • - ìˆ˜ì„ ì•„í‚¤í…íŠ¸ ì œì–¸ ë°˜ì˜"""

    # ë°ì´í„° ì„¤ì •
    data_path: str = 'data/smp/smp_5years_epsis.csv'
    output_dir: str = 'models/smp_advanced'

    # ê¸°ê°„ ì„¤ì • (2022-2024 2ë…„ì¹˜)
    train_start: str = '2022-01-01'
    train_end: str = '2024-12-31'

    # ì‹œí€€ìŠ¤ ì„¤ì •
    input_hours: int = 48        # 48ì‹œê°„ ì…ë ¥ (2ì¼)
    output_hours: int = 24       # 24ì‹œê°„ ì˜ˆì¸¡

    # ê²½ëŸ‰í™” ëª¨ë¸ ì„¤ì • (íŒŒë¼ë¯¸í„° ì•½ 1/5)
    hidden_size: int = 64        # 128 â†’ 64 (ê²½ëŸ‰í™”)
    num_layers: int = 2          # 3 â†’ 2 (ê²½ëŸ‰í™”)
    dropout: float = 0.3         # ê³¼ì í•© ë°©ì§€
    bidirectional: bool = True

    # Quantile ì„¤ì •
    quantiles: List[float] = field(default_factory=lambda: [0.1, 0.5, 0.9])

    # í•™ìŠµ ì„¤ì •
    batch_size: int = 64
    pretrain_epochs: int = 50    # Pre-train (í•©ì„± ë°ì´í„°)
    finetune_epochs: int = 100   # Fine-tune (ì‹¤ì œ ë°ì´í„°)
    learning_rate: float = 0.001
    finetune_lr: float = 0.0001  # Fine-tuneì‹œ ë” ì‘ì€ í•™ìŠµë¥ 
    patience: int = 20

    # Walk-forward ì„¤ì •
    n_splits: int = 5            # 5-fold walk-forward
    train_window: int = 365 * 24 # 1ë…„ í•™ìŠµ ìœˆë„ìš° (ì‹œê°„ ë‹¨ìœ„)
    test_window: int = 30 * 24   # 1ë‹¬ í…ŒìŠ¤íŠ¸ ìœˆë„ìš°

    # Noise Injection ì„¤ì •
    noise_std: float = 0.02      # 2% ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ
    noise_prob: float = 0.5      # 50% í™•ë¥ ë¡œ ì ìš©

    # ARIMA ì„¤ì •
    use_arima_ensemble: bool = True
    arima_weight: float = 0.3    # ARIMA ì•™ìƒë¸” ê°€ì¤‘ì¹˜

    # XAI ì„¤ì •
    save_attention_maps: bool = True


# =============================================================================
# Data Pipeline
# =============================================================================
class SMPDataPipeline:
    """SMP ë°ì´í„° íŒŒì´í”„ë¼ì¸ - ìˆ˜ì„ ì•„í‚¤í…íŠ¸ ê¶Œì¥ êµ¬ì¡°

    [Raw Data] â†’ [Validation] â†’ [Cleaning] â†’ [Transformation] â†’ [Feature Store]
    """

    def __init__(self, config: TrainingConfig):
        self.config = config
        self.scaler = None
        self.feature_names = []

    def load_data(self) -> pd.DataFrame:
        """ì‹¤ì œ EPSIS ë°ì´í„° ë¡œë“œ"""
        logger.info(f"ë°ì´í„° ë¡œë“œ: {self.config.data_path}")

        df = pd.read_csv(self.config.data_path)

        # íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì‹±
        df['datetime'] = pd.to_datetime(df['timestamp'].str.replace(' 24:00', ' 00:00'))

        # 24:00 ì²˜ë¦¬ (ë‹¤ìŒ ë‚ ë¡œ)
        mask_24 = df['timestamp'].str.contains('24:00', na=False)
        df.loc[mask_24, 'datetime'] = df.loc[mask_24, 'datetime'] + pd.Timedelta(days=1)

        # ì •ë ¬
        df = df.sort_values('datetime').reset_index(drop=True)

        # ê¸°ê°„ í•„í„°ë§ (2022-2024)
        start_date = pd.to_datetime(self.config.train_start)
        end_date = pd.to_datetime(self.config.train_end)
        df = df[(df['datetime'] >= start_date) & (df['datetime'] <= end_date)].copy()

        # ìœ íš¨ ë°ì´í„° í•„í„°ë§
        df = df[df['smp_mainland'] > 0].copy()

        logger.info(f"  ê¸°ê°„: {df['datetime'].min()} ~ {df['datetime'].max()}")
        logger.info(f"  ë ˆì½”ë“œ: {len(df):,}ê±´")

        return df

    def create_features(self, df: pd.DataFrame) -> np.ndarray:
        """í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ - ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ ì„¤ê³„

        í•µì‹¬ ì›ì¹™: ë¯¸ë˜ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í”¼ì²˜ë§Œ ìƒì„±
        """
        features = []
        smp = df['smp_mainland'].values

        # === 1. ê¸°ë³¸ ê°€ê²© í”¼ì²˜ ===
        features.append(smp)                         # ìœ¡ì§€ SMP (íƒ€ê²Ÿ)
        features.append(df['smp_jeju'].values)       # ì œì£¼ SMP
        features.append(df['smp_max'].values)        # ìµœê³ ê°€
        features.append(df['smp_min'].values)        # ìµœì €ê°€
        self.feature_names.extend(['smp_mainland', 'smp_jeju', 'smp_max', 'smp_min'])

        # === 2. ì‹œê°„ ìˆœí™˜ í”¼ì²˜ (ì£¼ê¸°ì„± ìº¡ì²˜) ===
        hour = df['hour'].values
        features.append(np.sin(2 * np.pi * hour / 24))
        features.append(np.cos(2 * np.pi * hour / 24))
        self.feature_names.extend(['hour_sin', 'hour_cos'])

        # === 3. ìš”ì¼/ì£¼ë§ í”¼ì²˜ ===
        day_of_week = df['datetime'].dt.dayofweek.values
        features.append(np.sin(2 * np.pi * day_of_week / 7))
        features.append(np.cos(2 * np.pi * day_of_week / 7))
        features.append((day_of_week >= 5).astype(float))
        self.feature_names.extend(['dow_sin', 'dow_cos', 'is_weekend'])

        # === 4. ì›”/ê³„ì ˆ í”¼ì²˜ ===
        month = df['datetime'].dt.month.values
        features.append(np.sin(2 * np.pi * month / 12))
        features.append(np.cos(2 * np.pi * month / 12))
        self.feature_names.extend(['month_sin', 'month_cos'])

        # ê³„ì ˆ (ì „ë ¥ ìˆ˜ìš” íŒ¨í„´)
        is_summer = ((month >= 6) & (month <= 8)).astype(float)
        is_winter = ((month == 12) | (month <= 2)).astype(float)
        features.append(is_summer)
        features.append(is_winter)
        self.feature_names.extend(['is_summer', 'is_winter'])

        # === 5. í”¼í¬ ì‹œê°„ëŒ€ í”¼ì²˜ ===
        peak_morning = ((hour >= 9) & (hour <= 12)).astype(float)
        peak_evening = ((hour >= 17) & (hour <= 21)).astype(float)
        off_peak = ((hour >= 1) & (hour <= 6)).astype(float)
        features.append(peak_morning)
        features.append(peak_evening)
        features.append(off_peak)
        self.feature_names.extend(['peak_morning', 'peak_evening', 'off_peak'])

        # === 6. ê³¼ê±° í†µê³„ í”¼ì²˜ (ëˆ„ìˆ˜ ì—†ìŒ - rollingì€ ê³¼ê±°ë§Œ ì‚¬ìš©) ===
        smp_series = pd.Series(smp)

        # Lag í”¼ì²˜ (ì´ì „ ì‹œì  ê°’)
        for lag in [1, 6, 12, 24]:
            lag_values = smp_series.shift(lag).fillna(method='bfill').values
            features.append(lag_values)
            self.feature_names.append(f'smp_lag_{lag}')

        # ì´ë™ í‰ê·  (ê³¼ê±°ë§Œ ì‚¬ìš©)
        ma_6 = smp_series.rolling(6, min_periods=1).mean().values
        ma_24 = smp_series.rolling(24, min_periods=1).mean().values
        features.append(ma_6)
        features.append(ma_24)
        self.feature_names.extend(['smp_ma_6', 'smp_ma_24'])

        # ì´ë™ í‘œì¤€í¸ì°¨ (ë³€ë™ì„±)
        std_24 = smp_series.rolling(24, min_periods=1).std().fillna(0).values
        features.append(std_24)
        self.feature_names.append('smp_std_24')

        # ë³€í™”ëŸ‰
        diff_1 = smp_series.diff().fillna(0).values
        diff_24 = smp_series.diff(24).fillna(0).values
        features.append(diff_1)
        features.append(diff_24)
        self.feature_names.extend(['smp_diff_1', 'smp_diff_24'])

        # ìŠ¤íƒ
        feature_array = np.column_stack(features)
        feature_array = np.nan_to_num(feature_array, nan=0.0, posinf=0.0, neginf=0.0)

        logger.info(f"í”¼ì²˜ ìƒì„± ì™„ë£Œ: {feature_array.shape[1]}ê°œ í”¼ì²˜")
        logger.info(f"  í”¼ì²˜ ëª©ë¡: {self.feature_names[:10]}...")

        return feature_array

    def normalize(self, data: np.ndarray, fit: bool = True) -> np.ndarray:
        """ì •ê·œí™”"""
        if fit:
            self.scaler = MinMaxScaler(feature_range=(0, 1))
            normalized = self.scaler.fit_transform(data)
        else:
            normalized = self.scaler.transform(data)
        return normalized

    def inverse_transform_smp(self, smp_normalized: np.ndarray) -> np.ndarray:
        """SMP ì—­ì •ê·œí™” (ì²« ë²ˆì§¸ í”¼ì²˜)"""
        smp_min = self.scaler.data_min_[0]
        smp_max = self.scaler.data_max_[0]
        return smp_normalized * (smp_max - smp_min) + smp_min


# =============================================================================
# Dataset with Noise Injection
# =============================================================================
class SMPDataset(Dataset):
    """SMP ì‹œê³„ì—´ ë°ì´í„°ì…‹ with ë…¸ì´ì¦ˆ ì£¼ì…"""

    def __init__(
        self,
        data: np.ndarray,
        input_hours: int = 48,
        output_hours: int = 24,
        noise_std: float = 0.0,
        noise_prob: float = 0.0,
        training: bool = True
    ):
        self.data = torch.FloatTensor(data)
        self.input_hours = input_hours
        self.output_hours = output_hours
        self.noise_std = noise_std
        self.noise_prob = noise_prob
        self.training = training

        # ìœ íš¨ ì¸ë±ìŠ¤ ìƒì„±
        total_len = len(data)
        self.indices = list(range(total_len - input_hours - output_hours + 1))

    def __len__(self):
        return len(self.indices)

    def __getitem__(self, idx):
        start = self.indices[idx]
        x = self.data[start:start + self.input_hours].clone()
        y = self.data[start + self.input_hours:start + self.input_hours + self.output_hours, 0].clone()

        # í•™ìŠµ ì‹œ ë…¸ì´ì¦ˆ ì£¼ì… (Data Augmentation)
        if self.training and self.noise_std > 0 and np.random.random() < self.noise_prob:
            noise = torch.randn_like(x) * self.noise_std
            x = x + noise
            x = torch.clamp(x, 0, 1)  # ì •ê·œí™” ë²”ìœ„ ìœ ì§€

        return x, y


# =============================================================================
# Lightweight Model with Enhanced Interpretability
# =============================================================================
class LightweightSMPModel(nn.Module):
    """ê²½ëŸ‰í™”ëœ SMP ì˜ˆì¸¡ ëª¨ë¸ - ì¼ë°˜í™” ì„±ëŠ¥ ê°•í™”

    ìˆ˜ì„ ì•„í‚¤í…íŠ¸ ì œì–¸:
    - íŒŒë¼ë¯¸í„°ë¥¼ 1/5 ìˆ˜ì¤€ìœ¼ë¡œ ì¤„ì—¬ ì¼ë°˜í™” ì„±ëŠ¥ ê°•ì œë¡œ ë†’ì„
    - Attention ê°€ì¤‘ì¹˜ ëª¨ë‹ˆí„°ë§ì„ í†µí•œ í•´ì„ ê°€ëŠ¥ì„± í™•ë³´
    """

    def __init__(
        self,
        input_size: int,
        hidden_size: int = 64,
        num_layers: int = 2,
        dropout: float = 0.3,
        bidirectional: bool = True,
        prediction_hours: int = 24,
        quantiles: List[float] = None
    ):
        super().__init__()

        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.dropout = dropout
        self.bidirectional = bidirectional
        self.prediction_hours = prediction_hours
        self.quantiles = quantiles or [0.1, 0.5, 0.9]

        # Input Layer Norm
        self.input_norm = nn.LayerNorm(input_size)

        # LSTM Encoder (ê²½ëŸ‰í™”)
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=bidirectional
        )

        lstm_output_size = hidden_size * 2 if bidirectional else hidden_size

        # Temporal Attention (XAI í•µì‹¬)
        self.attention_query = nn.Linear(lstm_output_size, hidden_size // 2)
        self.attention_key = nn.Linear(lstm_output_size, hidden_size // 2)
        self.attention_value = nn.Linear(lstm_output_size, hidden_size)
        self.attention_scale = np.sqrt(hidden_size // 2)

        # Shared Feature Extractor (ê²½ëŸ‰í™”)
        self.shared_fc = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.GELU(),
            nn.Dropout(dropout)
        )

        # Quantile Heads
        self.quantile_heads = nn.ModuleList([
            nn.Linear(hidden_size // 2, prediction_hours)
            for _ in self.quantiles
        ])

        # Point Estimate Head (ì¤‘ì•™ê°’)
        self.point_head = nn.Linear(hidden_size // 2, prediction_hours)

        self._init_weights()

    def _init_weights(self):
        """Xavier/Kaiming ì´ˆê¸°í™”"""
        for name, param in self.lstm.named_parameters():
            if 'weight_ih' in name:
                nn.init.xavier_uniform_(param.data)
            elif 'weight_hh' in name:
                nn.init.orthogonal_(param.data)
            elif 'bias' in name:
                param.data.fill_(0)
                n = param.size(0)
                param.data[n//4:n//2].fill_(1)  # Forget gate

    def forward(
        self,
        x: torch.Tensor,
        return_attention: bool = False,
        return_quantiles: bool = False
    ) -> torch.Tensor:
        """ìˆœì „íŒŒ

        Args:
            x: (batch, seq_len, input_size)
            return_attention: Attention ê°€ì¤‘ì¹˜ ë°˜í™˜
            return_quantiles: ë¶„ìœ„ìˆ˜ ì˜ˆì¸¡ê°’ ë°˜í™˜

        Returns:
            output: (batch, prediction_hours) - ì¤‘ì•™ê°’ ì˜ˆì¸¡
            attention_weights: (batch, seq_len) - if return_attention
            quantiles: dict of (batch, prediction_hours) - if return_quantiles
        """
        batch_size = x.size(0)

        # Input normalization
        x = self.input_norm(x)

        # LSTM encoding
        lstm_out, _ = self.lstm(x)  # (batch, seq_len, lstm_output_size)

        # Self-Attention
        Q = self.attention_query(lstm_out)
        K = self.attention_key(lstm_out)
        V = self.attention_value(lstm_out)

        # Attention scores
        attention_scores = torch.bmm(Q, K.transpose(1, 2)) / self.attention_scale
        attention_weights = F.softmax(attention_scores, dim=-1)

        # Context vector
        context = torch.bmm(attention_weights, V)  # (batch, seq_len, hidden)
        context = context.mean(dim=1)  # Global average pooling

        # Feature extraction
        features = self.shared_fc(context)

        # Point estimate (median)
        point_output = self.point_head(features)

        # Prepare return values
        result = {'point': point_output}

        if return_quantiles:
            quantile_outputs = {}
            for i, q in enumerate(self.quantiles):
                quantile_outputs[f'q{int(q*100)}'] = self.quantile_heads[i](features)
            result['quantiles'] = quantile_outputs

        if return_attention:
            # ì‹œí€€ìŠ¤ë³„ í‰ê·  attention
            result['attention'] = attention_weights.mean(dim=1)  # (batch, seq_len)

        if return_attention or return_quantiles:
            return result

        return point_output

    def get_num_parameters(self) -> int:
        """íŒŒë¼ë¯¸í„° ìˆ˜"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)


# =============================================================================
# Loss Functions
# =============================================================================
class QuantileLoss(nn.Module):
    """Pinball Loss for Quantile Regression"""

    def __init__(self, quantiles: List[float]):
        super().__init__()
        self.quantiles = quantiles

    def forward(self, predictions: Dict[str, torch.Tensor], targets: torch.Tensor) -> torch.Tensor:
        """
        Args:
            predictions: {'q10': tensor, 'q50': tensor, 'q90': tensor}
            targets: (batch, prediction_hours)
        """
        total_loss = 0.0

        for q in self.quantiles:
            pred = predictions[f'q{int(q*100)}']
            errors = targets - pred
            loss = torch.max(q * errors, (q - 1) * errors)
            total_loss += loss.mean()

        return total_loss / len(self.quantiles)


class CombinedLoss(nn.Module):
    """ê²°í•© ì†ì‹¤ í•¨ìˆ˜: MSE + Quantile Loss"""

    def __init__(self, quantiles: List[float], mse_weight: float = 0.5):
        super().__init__()
        self.mse_loss = nn.MSELoss()
        self.quantile_loss = QuantileLoss(quantiles)
        self.mse_weight = mse_weight

    def forward(
        self,
        point_pred: torch.Tensor,
        quantile_preds: Dict[str, torch.Tensor],
        targets: torch.Tensor
    ) -> Tuple[torch.Tensor, Dict[str, float]]:
        """
        Returns:
            total_loss: ê²°í•© ì†ì‹¤
            loss_dict: ê°œë³„ ì†ì‹¤ ë”•ì…”ë„ˆë¦¬
        """
        mse = self.mse_loss(point_pred, targets)
        quantile = self.quantile_loss(quantile_preds, targets)

        total = self.mse_weight * mse + (1 - self.mse_weight) * quantile

        return total, {
            'mse': mse.item(),
            'quantile': quantile.item(),
            'total': total.item()
        }


# =============================================================================
# Walk-Forward Validation
# =============================================================================
class WalkForwardValidator:
    """Walk-Forward ì‹œê³„ì—´ êµì°¨ê²€ì¦

    ì‹œê°„ìˆœìœ¼ë¡œ í•™ìŠµ â†’ ê²€ì¦ì„ ë°˜ë³µí•˜ì—¬ ì‹¤ì œ ìš´ìš© ì‹œë‚˜ë¦¬ì˜¤ ì‹œë®¬ë ˆì´ì…˜
    """

    def __init__(
        self,
        n_splits: int = 5,
        train_window: int = 365 * 24,
        test_window: int = 30 * 24,
        gap: int = 24  # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ê°„ ê°­ (ì •ë³´ ëˆ„ìˆ˜ ë°©ì§€)
    ):
        self.n_splits = n_splits
        self.train_window = train_window
        self.test_window = test_window
        self.gap = gap

    def split(self, n_samples: int) -> List[Tuple[np.ndarray, np.ndarray]]:
        """Walk-forward ë¶„í•  ìƒì„±

        Returns:
            List of (train_indices, test_indices) tuples
        """
        splits = []

        # ìµœì†Œ í•„ìš” ë°ì´í„°
        min_samples = self.train_window + self.gap + self.test_window

        if n_samples < min_samples:
            logger.warning(f"ë°ì´í„° ë¶€ì¡±: {n_samples} < {min_samples}. ë‹¨ì¼ ë¶„í•  ì‚¬ìš©.")
            train_end = int(n_samples * 0.7)
            return [(np.arange(train_end), np.arange(train_end, n_samples))]

        # Walk-forward ë¶„í• 
        step = (n_samples - min_samples) // self.n_splits

        for i in range(self.n_splits):
            train_start = i * step
            train_end = train_start + self.train_window
            test_start = train_end + self.gap
            test_end = min(test_start + self.test_window, n_samples)

            if test_end > test_start:
                train_idx = np.arange(train_start, train_end)
                test_idx = np.arange(test_start, test_end)
                splits.append((train_idx, test_idx))

        return splits


# =============================================================================
# ARIMA Component for Ensemble
# =============================================================================
class ARIMAComponent:
    """ARIMA í†µê³„ ëª¨ë¸ ì»´í¬ë„ŒíŠ¸

    ë”¥ëŸ¬ë‹ ëª¨ë¸ê³¼ ì•™ìƒë¸”í•˜ì—¬ êµ¬ì¡°ì  ì•ˆì •ì„± í™•ë³´
    """

    def __init__(self, order: Tuple[int, int, int] = (2, 1, 2)):
        self.order = order
        self.model = None

    def fit_predict(
        self,
        train_data: np.ndarray,
        forecast_steps: int = 24
    ) -> np.ndarray:
        """ARIMA í•™ìŠµ ë° ì˜ˆì¸¡

        Args:
            train_data: í•™ìŠµ ì‹œê³„ì—´ (1D array)
            forecast_steps: ì˜ˆì¸¡ ìŠ¤í… ìˆ˜

        Returns:
            predictions: ì˜ˆì¸¡ê°’
        """
        try:
            from statsmodels.tsa.arima.model import ARIMA

            # ìµœê·¼ ë°ì´í„°ë§Œ ì‚¬ìš© (íš¨ìœ¨ì„±)
            recent_data = train_data[-720:]  # ìµœê·¼ 30ì¼

            model = ARIMA(recent_data, order=self.order)
            fitted = model.fit()
            forecast = fitted.forecast(steps=forecast_steps)

            return forecast

        except Exception as e:
            logger.warning(f"ARIMA ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            # Fallback: ë§ˆì§€ë§‰ 24ì‹œê°„ í‰ê· 
            return np.full(forecast_steps, train_data[-24:].mean())


# =============================================================================
# XAI Pipeline - Attention Analyzer
# =============================================================================
class AttentionAnalyzer:
    """Attention ê¸°ë°˜ í•´ì„ ê°€ëŠ¥ì„± ë¶„ì„

    ìˆ˜ì„ ì•„í‚¤í…íŠ¸ ì œì–¸:
    - Attention Score ì‹œê°í™”ë¡œ ë°ì´í„° ëˆ„ìˆ˜ ê²€ì¦
    - í”¼ì²˜ ì¤‘ìš”ë„ ìƒì‹œ ê°ì‹œ
    """

    def __init__(self, feature_names: List[str]):
        self.feature_names = feature_names
        self.attention_history = []

    def analyze(
        self,
        model: nn.Module,
        sample_batch: torch.Tensor,
        device: torch.device
    ) -> Dict[str, Any]:
        """Attention ë¶„ì„

        Returns:
            analysis: Attention ë¶„ì„ ê²°ê³¼
        """
        model.eval()

        with torch.no_grad():
            sample_batch = sample_batch.to(device)
            result = model(sample_batch, return_attention=True, return_quantiles=True)

        attention_weights = result['attention'].cpu().numpy()  # (batch, seq_len)

        # ì‹œê°„ë³„ í‰ê·  Attention
        avg_attention = attention_weights.mean(axis=0)

        # í”¼í¬ ì‹œê°„ëŒ€ ë¶„ì„
        peak_indices = np.argsort(avg_attention)[-5:]  # Top 5

        analysis = {
            'avg_attention': avg_attention.tolist(),
            'peak_timesteps': peak_indices.tolist(),
            'attention_entropy': float(-np.sum(avg_attention * np.log(avg_attention + 1e-8))),
            'attention_concentration': float(np.max(avg_attention))
        }

        self.attention_history.append(analysis)

        return analysis

    def check_leakage_risk(self, analysis: Dict[str, Any]) -> str:
        """ë°ì´í„° ëˆ„ìˆ˜ ìœ„í—˜ ì²´í¬

        Returns:
            risk_level: 'low', 'medium', 'high'
        """
        concentration = analysis['attention_concentration']

        if concentration > 0.5:
            return 'high'  # íŠ¹ì • ì‹œì ì— ê³¼ë„í•˜ê²Œ ì§‘ì¤‘
        elif concentration > 0.3:
            return 'medium'
        else:
            return 'low'


# =============================================================================
# Drift Detector
# =============================================================================
class DriftDetector:
    """ì„±ëŠ¥ ë“œë¦¬í”„íŠ¸ íƒì§€

    í•©ì„± ë°ì´í„°ì™€ ì‹¤ì œ ë°ì´í„° ê²½ê³„ì—ì„œì˜ ì„±ëŠ¥ ë³€í™” ëª¨ë‹ˆí„°ë§
    """

    def __init__(self, window_size: int = 24 * 7):
        self.window_size = window_size
        self.error_history = []

    def update(self, errors: np.ndarray):
        """ì—ëŸ¬ ì—…ë°ì´íŠ¸"""
        self.error_history.extend(errors.tolist())

    def detect_drift(self) -> Dict[str, Any]:
        """ë“œë¦¬í”„íŠ¸ íƒì§€

        Returns:
            drift_info: ë“œë¦¬í”„íŠ¸ ì •ë³´
        """
        if len(self.error_history) < self.window_size * 2:
            return {'detected': False, 'reason': 'insufficient_data'}

        recent = np.array(self.error_history[-self.window_size:])
        previous = np.array(self.error_history[-2*self.window_size:-self.window_size])

        # í†µê³„ì  ë¹„êµ
        recent_mean = np.mean(np.abs(recent))
        previous_mean = np.mean(np.abs(previous))

        drift_ratio = recent_mean / (previous_mean + 1e-8)

        detected = drift_ratio > 1.5 or drift_ratio < 0.5

        return {
            'detected': detected,
            'drift_ratio': float(drift_ratio),
            'recent_mae': float(recent_mean),
            'previous_mae': float(previous_mean)
        }


# =============================================================================
# Training Engine
# =============================================================================
class TrainingEngine:
    """ê³ ë„í™”ëœ í•™ìŠµ ì—”ì§„ - Sim-to-Real ì „ëµ"""

    def __init__(self, config: TrainingConfig):
        self.config = config
        self.device = get_device()
        self.pipeline = SMPDataPipeline(config)
        self.drift_detector = DriftDetector()
        self.attention_analyzer = None

    def run_training_epoch(
        self,
        model: nn.Module,
        loader: DataLoader,
        optimizer: torch.optim.Optimizer,
        loss_fn: CombinedLoss,
        is_training: bool = True
    ) -> Dict[str, float]:
        """ë‹¨ì¼ ì—í­ ì‹¤í–‰"""
        if is_training:
            model.train()
        else:
            model.eval()

        total_losses = {'mse': 0, 'quantile': 0, 'total': 0}
        n_batches = 0

        context = torch.no_grad() if not is_training else torch.enable_grad()

        with context:
            for batch_x, batch_y in loader:
                batch_x = batch_x.to(self.device)
                batch_y = batch_y.to(self.device)

                if is_training:
                    optimizer.zero_grad()

                result = model(batch_x, return_quantiles=True)
                point_pred = result['point']
                quantile_preds = result['quantiles']

                loss, loss_dict = loss_fn(point_pred, quantile_preds, batch_y)

                if is_training:
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    optimizer.step()

                for k in total_losses:
                    total_losses[k] += loss_dict[k]
                n_batches += 1

        return {k: v / n_batches for k, v in total_losses.items()}

    def train_model(
        self,
        model: nn.Module,
        train_loader: DataLoader,
        val_loader: DataLoader,
        epochs: int,
        learning_rate: float,
        patience: int,
        phase: str = 'pretrain'
    ) -> Dict[str, Any]:
        """ëª¨ë¸ í•™ìŠµ

        Args:
            phase: 'pretrain' or 'finetune'
        """
        model = model.to(self.device)

        loss_fn = CombinedLoss(self.config.quantiles)
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=learning_rate,
            weight_decay=1e-4
        )
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=5
        )

        best_val_loss = float('inf')
        best_state = None
        patience_counter = 0
        history = {'train': [], 'val': []}

        logger.info(f"[{phase.upper()}] í•™ìŠµ ì‹œì‘ (epochs={epochs}, lr={learning_rate})")

        for epoch in range(epochs):
            # Training
            train_losses = self.run_training_epoch(
                model, train_loader, optimizer, loss_fn, is_training=True
            )

            # Validation
            val_losses = self.run_training_epoch(
                model, val_loader, optimizer, loss_fn, is_training=False
            )

            scheduler.step(val_losses['total'])

            history['train'].append(train_losses)
            history['val'].append(val_losses)

            # Logging
            if (epoch + 1) % 10 == 0 or epoch == 0:
                logger.info(
                    f"  Epoch {epoch+1:3d}/{epochs} | "
                    f"Train: {train_losses['total']:.6f} | "
                    f"Val: {val_losses['total']:.6f}"
                )

            # Early stopping
            if val_losses['total'] < best_val_loss:
                best_val_loss = val_losses['total']
                best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}
                patience_counter = 0
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    logger.info(f"  Early stopping at epoch {epoch+1}")
                    break

        # Restore best
        if best_state:
            model.load_state_dict(best_state)

        return {
            'history': history,
            'best_val_loss': best_val_loss,
            'epochs_trained': epoch + 1
        }

    def evaluate(
        self,
        model: nn.Module,
        test_loader: DataLoader
    ) -> Dict[str, float]:
        """ëª¨ë¸ í‰ê°€"""
        model.eval()
        model = model.to(self.device)

        all_preds = []
        all_targets = []
        all_q10 = []
        all_q90 = []

        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                batch_x = batch_x.to(self.device)

                result = model(batch_x, return_quantiles=True)

                all_preds.append(result['point'].cpu().numpy())
                all_targets.append(batch_y.numpy())
                all_q10.append(result['quantiles']['q10'].cpu().numpy())
                all_q90.append(result['quantiles']['q90'].cpu().numpy())

        preds = np.concatenate(all_preds, axis=0)
        targets = np.concatenate(all_targets, axis=0)
        q10 = np.concatenate(all_q10, axis=0)
        q90 = np.concatenate(all_q90, axis=0)

        # ì—­ì •ê·œí™”
        preds_real = self.pipeline.inverse_transform_smp(preds)
        targets_real = self.pipeline.inverse_transform_smp(targets)
        q10_real = self.pipeline.inverse_transform_smp(q10)
        q90_real = self.pipeline.inverse_transform_smp(q90)

        # ë©”íŠ¸ë¦­
        mae = np.mean(np.abs(preds_real - targets_real))
        rmse = np.sqrt(np.mean((preds_real - targets_real) ** 2))
        mape = np.mean(np.abs((targets_real - preds_real) / (targets_real + 1e-8))) * 100

        ss_res = np.sum((targets_real - preds_real) ** 2)
        ss_tot = np.sum((targets_real - np.mean(targets_real)) ** 2)
        r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0

        # êµ¬ê°„ ì»¤ë²„ë¦¬ì§€ (95% CI)
        in_interval = (targets_real >= q10_real) & (targets_real <= q90_real)
        coverage = np.mean(in_interval) * 100

        # í‰ê·  êµ¬ê°„ í­
        interval_width = np.mean(q90_real - q10_real)

        return {
            'mae': float(mae),
            'rmse': float(rmse),
            'mape': float(mape),
            'r2': float(r2),
            'coverage_80': float(coverage),
            'interval_width': float(interval_width)
        }

    def run_walk_forward_cv(
        self,
        model_class: type,
        model_kwargs: Dict[str, Any],
        data: np.ndarray
    ) -> List[Dict[str, float]]:
        """Walk-Forward êµì°¨ê²€ì¦ ì‹¤í–‰"""
        validator = WalkForwardValidator(
            n_splits=self.config.n_splits,
            train_window=self.config.train_window,
            test_window=self.config.test_window
        )

        splits = validator.split(len(data))
        fold_results = []

        logger.info(f"Walk-Forward CV: {len(splits)} folds")

        for fold_idx, (train_idx, test_idx) in enumerate(splits):
            logger.info(f"  Fold {fold_idx + 1}/{len(splits)}")

            # ë°ì´í„° ë¶„í• 
            train_data = data[train_idx]
            test_data = data[test_idx]

            # ë°ì´í„°ì…‹ ìƒì„±
            train_dataset = SMPDataset(
                train_data,
                self.config.input_hours,
                self.config.output_hours,
                self.config.noise_std,
                self.config.noise_prob,
                training=True
            )
            test_dataset = SMPDataset(
                test_data,
                self.config.input_hours,
                self.config.output_hours,
                training=False
            )

            if len(train_dataset) < 10 or len(test_dataset) < 5:
                logger.warning(f"    Fold {fold_idx + 1} ë°ì´í„° ë¶€ì¡±, ê±´ë„ˆëœ€")
                continue

            train_loader = DataLoader(train_dataset, batch_size=self.config.batch_size, shuffle=True)
            test_loader = DataLoader(test_dataset, batch_size=self.config.batch_size)

            # ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
            model = model_class(**model_kwargs)

            self.train_model(
                model,
                train_loader,
                test_loader,  # ê°„ë‹¨íˆ í…ŒìŠ¤íŠ¸ë¥¼ ê²€ì¦ìœ¼ë¡œë„ ì‚¬ìš©
                epochs=30,    # CVëŠ” ë¹ ë¥´ê²Œ
                learning_rate=self.config.learning_rate,
                patience=10,
                phase=f'cv_fold_{fold_idx+1}'
            )

            # í‰ê°€
            metrics = self.evaluate(model, test_loader)
            fold_results.append(metrics)

            logger.info(f"    MAPE: {metrics['mape']:.2f}%, RÂ²: {metrics['r2']:.4f}")

        return fold_results


# =============================================================================
# Main Training Pipeline
# =============================================================================
def main():
    """ë©”ì¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸"""
    print("=" * 70)
    print("SMP Advanced Training Pipeline - Sim-to-Real Architecture")
    print("ìˆ˜ì„ ì•„í‚¤í…íŠ¸ ì œì–¸ ë°˜ì˜ ê³ ë„í™” ë²„ì „")
    print("=" * 70)

    config = TrainingConfig()
    engine = TrainingEngine(config)

    # =========================================================================
    # 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    # =========================================================================
    print("\n" + "=" * 70)
    print("Phase 1: ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (2022-2024)")
    print("=" * 70)

    df = engine.pipeline.load_data()
    features = engine.pipeline.create_features(df)
    normalized_data = engine.pipeline.normalize(features, fit=True)

    print(f"\n  ì „ì²´ ë°ì´í„°: {len(normalized_data):,}ê±´")
    print(f"  í”¼ì²˜ ìˆ˜: {features.shape[1]}ê°œ")

    # =========================================================================
    # 2. ê²½ëŸ‰í™” ëª¨ë¸ ìƒì„±
    # =========================================================================
    print("\n" + "=" * 70)
    print("Phase 2: ê²½ëŸ‰í™” ëª¨ë¸ ìƒì„±")
    print("=" * 70)

    model_kwargs = {
        'input_size': features.shape[1],
        'hidden_size': config.hidden_size,
        'num_layers': config.num_layers,
        'dropout': config.dropout,
        'bidirectional': config.bidirectional,
        'prediction_hours': config.output_hours,
        'quantiles': config.quantiles
    }

    model = LightweightSMPModel(**model_kwargs)

    n_params = model.get_num_parameters()
    print(f"  ëª¨ë¸: LightweightSMPModel")
    print(f"  íŒŒë¼ë¯¸í„°: {n_params:,} ({n_params/1e6:.2f}M)")
    print(f"  Hidden Size: {config.hidden_size}")
    print(f"  Layers: {config.num_layers}")

    # =========================================================================
    # 3. Walk-Forward êµì°¨ê²€ì¦
    # =========================================================================
    print("\n" + "=" * 70)
    print("Phase 3: Walk-Forward êµì°¨ê²€ì¦")
    print("=" * 70)

    cv_results = engine.run_walk_forward_cv(
        LightweightSMPModel,
        model_kwargs,
        normalized_data
    )

    if cv_results:
        avg_mape = np.mean([r['mape'] for r in cv_results])
        avg_r2 = np.mean([r['r2'] for r in cv_results])
        avg_coverage = np.mean([r['coverage_80'] for r in cv_results])

        print(f"\n  CV í‰ê·  MAPE: {avg_mape:.2f}% (Â±{np.std([r['mape'] for r in cv_results]):.2f})")
        print(f"  CV í‰ê·  RÂ²: {avg_r2:.4f}")
        print(f"  CV í‰ê·  80% ì»¤ë²„ë¦¬ì§€: {avg_coverage:.1f}%")

    # =========================================================================
    # 4. ìµœì¢… ëª¨ë¸ í•™ìŠµ (ë…¸ì´ì¦ˆ ì£¼ì… í¬í•¨)
    # =========================================================================
    print("\n" + "=" * 70)
    print("Phase 4: ìµœì¢… ëª¨ë¸ í•™ìŠµ (Noise Injection ì ìš©)")
    print("=" * 70)

    # ë°ì´í„° ë¶„í•  (80/10/10)
    n_samples = len(normalized_data)
    train_end = int(n_samples * 0.8)
    val_end = int(n_samples * 0.9)

    train_data = normalized_data[:train_end]
    val_data = normalized_data[train_end:val_end]
    test_data = normalized_data[val_end:]

    # ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = SMPDataset(
        train_data,
        config.input_hours,
        config.output_hours,
        config.noise_std,      # 2% ë…¸ì´ì¦ˆ
        config.noise_prob,     # 50% í™•ë¥ 
        training=True
    )
    val_dataset = SMPDataset(
        val_data,
        config.input_hours,
        config.output_hours,
        training=False
    )
    test_dataset = SMPDataset(
        test_data,
        config.input_hours,
        config.output_hours,
        training=False
    )

    print(f"  í•™ìŠµ: {len(train_dataset):,}, ê²€ì¦: {len(val_dataset):,}, í…ŒìŠ¤íŠ¸: {len(test_dataset):,}")
    print(f"  Noise Injection: std={config.noise_std}, prob={config.noise_prob}")

    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=config.batch_size)
    test_loader = DataLoader(test_dataset, batch_size=config.batch_size)

    # ëª¨ë¸ í•™ìŠµ
    final_model = LightweightSMPModel(**model_kwargs)

    train_result = engine.train_model(
        final_model,
        train_loader,
        val_loader,
        epochs=config.finetune_epochs,
        learning_rate=config.learning_rate,
        patience=config.patience,
        phase='final'
    )

    print(f"\n  í•™ìŠµ ì™„ë£Œ: {train_result['epochs_trained']} epochs")
    print(f"  Best Val Loss: {train_result['best_val_loss']:.6f}")

    # =========================================================================
    # 5. ìµœì¢… í‰ê°€
    # =========================================================================
    print("\n" + "=" * 70)
    print("Phase 5: ìµœì¢… í‰ê°€")
    print("=" * 70)

    metrics = engine.evaluate(final_model, test_loader)

    print(f"\n  ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"     MAE:  {metrics['mae']:.2f} ì›/kWh")
    print(f"     RMSE: {metrics['rmse']:.2f} ì›/kWh")
    print(f"     MAPE: {metrics['mape']:.2f}%")
    print(f"     RÂ²:   {metrics['r2']:.4f}")
    print(f"     80% êµ¬ê°„ ì»¤ë²„ë¦¬ì§€: {metrics['coverage_80']:.1f}%")
    print(f"     í‰ê·  ì˜ˆì¸¡ êµ¬ê°„ í­: {metrics['interval_width']:.2f} ì›/kWh")

    # =========================================================================
    # 6. XAI ë¶„ì„
    # =========================================================================
    print("\n" + "=" * 70)
    print("Phase 6: XAI ë¶„ì„ (Attention í•´ì„)")
    print("=" * 70)

    analyzer = AttentionAnalyzer(engine.pipeline.feature_names)

    # ìƒ˜í”Œ ë°°ì¹˜ë¡œ ë¶„ì„
    sample_x, _ = next(iter(test_loader))
    analysis = analyzer.analyze(final_model, sample_x, engine.device)

    risk_level = analyzer.check_leakage_risk(analysis)

    print(f"  Attention Entropy: {analysis['attention_entropy']:.4f}")
    print(f"  Attention Concentration: {analysis['attention_concentration']:.4f}")
    print(f"  ë°ì´í„° ëˆ„ìˆ˜ ìœ„í—˜: {risk_level.upper()}")
    print(f"  ì£¼ìš” ì£¼ëª© ì‹œì : {analysis['peak_timesteps']}")

    # =========================================================================
    # 7. ARIMA ì•™ìƒë¸” (ì„ íƒì )
    # =========================================================================
    if config.use_arima_ensemble:
        print("\n" + "=" * 70)
        print("Phase 7: ARIMA ì•™ìƒë¸”")
        print("=" * 70)

        try:
            arima = ARIMAComponent(order=(2, 1, 2))

            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ì²« ë°°ì¹˜ë¡œ ì•™ìƒë¸” í…ŒìŠ¤íŠ¸
            sample_x, sample_y = next(iter(test_loader))
            sample_x = sample_x.to(engine.device)

            with torch.no_grad():
                result = final_model(sample_x, return_quantiles=True)
                lstm_pred = result['point'].cpu().numpy()[0]

            # ARIMA ì˜ˆì¸¡
            train_smp = train_data[:, 0]  # SMPë§Œ ì‚¬ìš©
            arima_pred = arima.fit_predict(train_smp, config.output_hours)

            # ì•™ìƒë¸”
            ensemble_pred = (
                (1 - config.arima_weight) * lstm_pred +
                config.arima_weight * arima_pred
            )

            # ì—­ì •ê·œí™”
            lstm_real = engine.pipeline.inverse_transform_smp(lstm_pred)
            arima_real = engine.pipeline.inverse_transform_smp(arima_pred)
            ensemble_real = engine.pipeline.inverse_transform_smp(ensemble_pred)
            actual_real = engine.pipeline.inverse_transform_smp(sample_y.numpy()[0])

            print(f"  LSTM MAE: {np.mean(np.abs(lstm_real - actual_real)):.2f}")
            print(f"  ARIMA MAE: {np.mean(np.abs(arima_real - actual_real)):.2f}")
            print(f"  Ensemble MAE: {np.mean(np.abs(ensemble_real - actual_real)):.2f}")
            print(f"  Ensemble Weight: LSTM={1-config.arima_weight:.1f}, ARIMA={config.arima_weight:.1f}")

        except Exception as e:
            logger.warning(f"ARIMA ì•™ìƒë¸” ì‹¤íŒ¨: {e}")

    # =========================================================================
    # 8. ëª¨ë¸ ì €ì¥
    # =========================================================================
    print("\n" + "=" * 70)
    print("Phase 8: ëª¨ë¸ ì €ì¥")
    print("=" * 70)

    output_dir = Path(config.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # ëª¨ë¸ ì €ì¥
    model_path = output_dir / 'smp_advanced_model.pt'
    torch.save({
        'model_state_dict': final_model.state_dict(),
        'model_kwargs': model_kwargs,
        'config': {
            'hidden_size': config.hidden_size,
            'num_layers': config.num_layers,
            'dropout': config.dropout,
            'quantiles': config.quantiles,
            'input_hours': config.input_hours,
            'output_hours': config.output_hours
        },
        'metrics': metrics,
        'cv_results': cv_results,
        'xai_analysis': analysis,
        'timestamp': datetime.now().isoformat()
    }, model_path)

    # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    scaler_path = output_dir / 'smp_advanced_scaler.npy'
    np.save(scaler_path, {
        'data_min_': engine.pipeline.scaler.data_min_,
        'data_max_': engine.pipeline.scaler.data_max_,
        'scale_': engine.pipeline.scaler.scale_,
        'feature_names': engine.pipeline.feature_names
    })

    # ë©”íŠ¸ë¦­ ì €ì¥
    metrics_path = output_dir / 'smp_advanced_metrics.json'
    with open(metrics_path, 'w') as f:
        json.dump({
            'final_metrics': metrics,
            'cv_results': cv_results,
            'config': {
                'hidden_size': config.hidden_size,
                'num_layers': config.num_layers,
                'n_params': n_params,
                'noise_std': config.noise_std,
                'train_period': f"{config.train_start} ~ {config.train_end}"
            },
            'xai_analysis': {
                'leakage_risk': risk_level,
                'attention_entropy': analysis['attention_entropy']
            },
            'timestamp': datetime.now().isoformat()
        }, f, indent=2)

    print(f"  ëª¨ë¸ ì €ì¥: {model_path}")
    print(f"  ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: {scaler_path}")
    print(f"  ë©”íŠ¸ë¦­ ì €ì¥: {metrics_path}")

    # =========================================================================
    # 9. ìµœì¢… ìš”ì•½
    # =========================================================================
    print("\n" + "=" * 70)
    print("í•™ìŠµ ì™„ë£Œ - ìµœì¢… ìš”ì•½")
    print("=" * 70)

    print(f"""
  ğŸ“ˆ ëª¨ë¸ ì„±ëŠ¥:
     â€¢ MAPE: {metrics['mape']:.2f}%
     â€¢ RÂ²: {metrics['r2']:.4f}
     â€¢ 80% ì»¤ë²„ë¦¬ì§€: {metrics['coverage_80']:.1f}%

  ğŸ”§ ëª¨ë¸ êµ¬ì„±:
     â€¢ íŒŒë¼ë¯¸í„°: {n_params:,}
     â€¢ ê²½ëŸ‰í™” ë¹„ìœ¨: ì•½ 1/5 (ê¸°ì¡´ 1M â†’ {n_params/1000:.0f}K)

  ğŸ¯ ì•„í‚¤í…ì²˜ íŠ¹ì§•:
     â€¢ Quantile Regression (10%, 50%, 90%)
     â€¢ Walk-forward CV ({len(cv_results)} folds)
     â€¢ Noise Injection (std={config.noise_std})
     â€¢ Attention-based XAI

  âš ï¸ ë°ì´í„° ëˆ„ìˆ˜ ìœ„í—˜: {risk_level.upper()}
    """)

    print("=" * 70)

    return final_model, metrics


if __name__ == "__main__":
    main()
